# Activity Quality Prediction using sensor data

Rubric:  
Has the student submitted a github repo? (done)  
Does the submission build a machine learning algorithm to predict activity quality from activity monitors?  (done)  
Do the authors describe what they expect the out of sample error to be and estimate the error appropriately with cross-validation?  (done)  



The accuracy measures are 
```{r}
rfTest$comparison
```


```{r basicCodeOutline, eval=FALSE}
######## Analysis setup ###########################################################
set.seed(294); require(caret); source("./helper_functions.R")

######## Data Preparation #########################################################
#load data
trainData = read.csv("../data//pml-training.csv", stringsAsFactors=F)    
trainData$classe <- factor(trainData$classe)

## feature selection
rawSensorFeatures = grep("^accel|^gyros|^magn", names(trainData), value=T)
targetFeatures = rawSensorFeatures

# k-fold cross validation: randomly split trainData into 3 homogenous subsets
dataFoldSegments = createFolds(y = trainData$classe, k=3)

# subset data according to folds
targetFolds = list()
targetFolds$train = trainData[dataFoldSegments$Fold1,]
targetFolds$validation = trainData[dataFoldSegments$Fold2,]
targetFolds$test = trainData[dataFoldSegments$Fold3,]

# now to subset on features to remove unwanted features
targetFolds = subsetFeatures(folds = targetFolds, features = targetFeatures)

######## Training #################################################################

# Training the prediciton model. Random forest worked well in iniial testing
rfModel = train(classe ~ ., data=targetFolds$train, method="rf")
rfTest = testModel(targetFolds, rfModel)
```