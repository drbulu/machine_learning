# Activity Quality Prediction using sensor data

## Outline  
<ol>
  <li><a href="#summary">Executive Summary</a></li>
  <li><a href="#walkthrough">Code Walkthrough</a></li>
  <ol>
    <li><a href="#loadData">Data preparation</a></li>
    <li><a href="#features">Feature selection and Model training</a></li>
    <li><a href="#results">Results</a></li>
  </ol>
</ol>

<h2 id="summary">Executive Summary</h2>  
<b>Background:</b>   
The aim of this project was to classify the way in which a particular activity was performed in order to provide feedback to potential user regarding how well they performed this activity. This is the logical step beyond using the data to identify what the activity is. The subtle challenge is that it is harder to classify *how well* an activity is being perfomed than to distinguish one type of activity from another. 
</br>
<b>Methodology:</b>   
For this analysis I split the training data set into 3 folds in order to generate training, validation and testing data subsets on which to train and validate my chosen machine learning (ML) approach. The ML method of choice was the <b>Random forest</b>, using the caret package implemention in R. 

### Fix
K-fold cross-validation  
Accuracy was used as the primary performance metric during the ML training and validation steps, and the average accuracy of the trained Random forrest model over the validation and test sets was used to get an estimate of the generalisation (out of sample) accuracy.
</br>
<b>Results:</b>
The methodology was used to fit a random model Random forest that had an accuracy of 96.4% and 96.7%, respectively, giving estimated out of sample accuracy of 96.6%, implying an expected out of sample error of roughly <b style="color:green;">3.5%</b>. 

Random forest (why?),  
K-fold cross-validation (random partitioning to avoid overfitting to individuals)  

### Do the authors describe what they expect the out of sample error to be and estimate the error appropriately with cross-validation?  (done)  

... I split data into 3 even partitions (enough data to fit model, not enough to overfit), trained model on one fold, then estimated the model on the other 2 folds.


<h2 id="walkthrough">Code Walkthrough</h2>  

Intro paragraph

(http://groupware.les.inf.puc-rio.br/har)

The training and testing of the model is described below. Note: for better code clarity, I separated the code into the main instructions (in the project_code.R file) and helper functions designed to streamline the analysis workflow (the helper_functions.R).

<h3 id="loadData">Data preparation</h3> 


```{r loadData, cache=TRUE}
######## Analysis setup ######################################################
set.seed(294); require(caret); source("./helper_functions.R")

######## Data Preparation ####################################################
#load data
trainData = read.csv("../data//pml-training.csv", stringsAsFactors=F)    
trainData$classe <- factor(trainData$classe)

# k-fold cross validation: randomly split trainData into 3 homogenous subsets
dataFoldSegments = createFolds(y = trainData$classe, k=3)

# subset data according to folds.
# This is a bit tedious, but conforms to the input 
targetFolds = list()
targetFolds$train = trainData[dataFoldSegments$Fold1,]
targetFolds$validation = trainData[dataFoldSegments$Fold2,]
targetFolds$test = trainData[dataFoldSegments$Fold3,]
```

<h3 id="features">Feature selection and Model training</h3> 

```{r featuresAndTrain, cache=TRUE}
######## feature selection ###################################################
# a) get column indices for desired features
rawSensorFeatures = grep("^accel|^gyros|^magn", names(trainData), value=T)
targetFeatures = rawSensorFeatures
# b) remove unwanted features from the data set
targetFolds <- subsetFeatures(folds = targetFolds, features = targetFeatures)

######## Model training and Cross validation #################################
# Cycle 1: train = train, validation = validation, test = test
rfModel1 = train(classe ~ ., data=targetFolds$train, method="rf")
rfTest1a = testModel(targetFolds$train, rfModel1)
rfTest1b = testModel(targetFolds$validation, rfModel1)
rfTest1c = testModel(targetFolds$test, rfModel1)
# Cycle 2: train = validation, validation = test, test = train
rfModel2 = train(classe ~ ., data=targetFolds$validation, method="rf")
rfTest2a = testModel(targetFolds$validation, rfModel2)
rfTest2b = testModel(targetFolds$test, rfModel2)
rfTest2c = testModel(targetFolds$train, rfModel2)
# Cycle 3: train = test, validation = train, test = validation
rfModel3 = train(classe ~ ., data=targetFolds$test, method="rf")
rfTest3a = testModel(targetFolds$test, rfModel3)
rfTest3b = testModel(targetFolds$train, rfModel3)
rfTest3c = testModel(targetFolds$validation, rfModel3)
```

<h3 id="results">Analysis results</h3> 

```{r results}
######## Summary of cross validation results #################################
accuracyTest = data.frame(rbind(
  cycle1 = c( rfTest1a$overall[1], rfTest1b$overall[1], rfTest1c$overall[1] ),
  cycle2 = c( rfTest2a$overall[1], rfTest2b$overall[1], rfTest2c$overall[1] ),
  cycle3 = c( rfTest3a$overall[1], rfTest3b$overall[1], rfTest3c$overall[1] )
  ), stringsAsFactors=F)
names(accuracyTest) <- c("train","validation", "test")

accVals = c(accuracyTest$validation, accuracyTest$test)
accMean = mean( accVals )
accSd = mean( accVals )

# test
accVals
accMean
accSd

```

